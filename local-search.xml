<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>CS336 Lecture 2 - Pytorch基础概念以及运作机制</title>
    <link href="/2026/02/22/Lecture2%20Pytorch%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5%E4%BB%A5%E5%8F%8A%E8%BF%90%E4%BD%9C%E6%9C%BA%E5%88%B6/"/>
    <url>/2026/02/22/Lecture2%20Pytorch%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5%E4%BB%A5%E5%8F%8A%E8%BF%90%E4%BD%9C%E6%9C%BA%E5%88%B6/</url>
    
    <content type="html"><![CDATA[<p>本讲深入讲解 PyTorch 框架的核心概念和底层运作机制。从模型训练的计算量分析入手，学习如何估算训练时间和显存占用。重点涵盖：张量（Tensor）的内存表示与不同精度类型（FP32、FP16、BF16、FP8）、GPU 上的张量操作与内存管理、以及 PyTorch 的高级操作库 einops。同时详细解析了深度学习中的计算量（FLOPs）和梯度计算原理，最后介绍了模型参数初始化、数据加载和优化器（特别是 AdaGrad）的实现细节。这些基础知识为后续高效实现和优化深度学习模型奠定坚实基础。</p><span id="more"></span><h3 id="0-模型训练计算"><a href="#0-模型训练计算" class="headerlink" title="0.模型训练计算"></a>0.模型训练计算</h3><p>question1 在1024个H100上用15T token训练一个70B的模型多久</p><ul><li>计算训练需要的总浮点运算数：6 * 参数量 * token数量<br><img src="/img/lecture2_pytorch/Pasted%20image%2020260216124423.png" alt="图片"></li></ul><p>question2 用8张H100，和AdamW可以训练的最大模型是多大（参数量）</p><ul><li>H100 有80GB显存</li><li>参数量是总显存量除以每个参数需要的字节数</li><li>这里算的比较简略，没有考虑激活值（取决于batchsize和sequence length）<br><img src="/img/lecture2_pytorch/Pasted%20image%2020260216124339.png" alt="图片"></li></ul><h3 id="1-Memory-accounting"><a href="#1-Memory-accounting" class="headerlink" title="1.Memory accounting"></a>1.Memory accounting</h3><h4 id="1-1-tensors-basics"><a href="#1-1-tensors-basics" class="headerlink" title="1.1 tensors_basics"></a>1.1 tensors_basics</h4><p>创建张量并初始化<br><img src="/img/lecture2_pytorch/Pasted%20image%2020260216124944.png" alt="图片"><br>也可以不初始化值，后续进行一些逻辑操作<br><img src="/img/lecture2_pytorch/Pasted%20image%2020260216125701.png" alt="图片"><br>张量都是浮点数，有不同的表达</p><ul><li>float32（全精度，FP32，单精度）（实际上有还有更大的，float64，但是一般dl都使用float32就够了）<ul><li>一位用于正负占位</li><li>8个bit用于指数</li><li>23个bit用于尾数<br>   <img src="/img/lecture2_pytorch/Pasted%20image%2020260216125845.png" alt="图片"><br>  例子<br>  <img src="/img/lecture2_pytorch/Pasted%20image%2020260216131435.png" alt="图片"></li></ul></li><li>float16（半精度，FP16）能表示的数的范围比较小，容易上溢（数值太大超出表示范围）和下溢（数值太小超出表示范围），只适合训练小模型<br>  <img src="/img/lecture2_pytorch/Pasted%20image%2020260216132653.png" alt="图片"></li><li>bfloat16（BF16） 用来进行计算。但是存储优化器和参数还是要用FP32，不让不太稳定<br>  <img src="/img/lecture2_pytorch/Pasted%20image%2020260216132939.png" alt="图片"></li><li>fp8 两种变体<br>  <img src="/img/lecture2_pytorch/Pasted%20image%2020260216133119.png" alt="图片"><br>&#x3D;&#x3D;总结&#x3D;&#x3D;</li><li>一般训练会使用FP32，但是需要的内存比较多</li><li>使用FP8、FP16甚至BF16（一般比较少）有风险，不稳定</li><li>可以使用混合精度训练，比如在简单的设计矩阵乘法的向前传播使用BF16，其它情况使用FP32是可以的。（提取出来，转化成BF16计算，然后转换回去，还有梯度缩放之类的）</li></ul><h4 id="1-2-memory-usage-of-tensor"><a href="#1-2-memory-usage-of-tensor" class="headerlink" title="1.2 memory usage of tensor"></a>1.2 memory usage of tensor</h4><p>内存用量&#x3D; 张量元素个数 *  每个元素大小（32位，4bytes,这里用FP32进行计算）<br><img src="/img/lecture2_pytorch/Pasted%20image%2020260216131948.png" alt="图片"><br>GPT3的前馈层（有四倍隐藏维度，维度是12288）<br><img src="/img/lecture2_pytorch/Pasted%20image%2020260216132313.png" alt="图片"></p><h3 id="2-Compute-accounting"><a href="#2-Compute-accounting" class="headerlink" title="2.Compute accounting"></a>2.Compute accounting</h3><h4 id="2-1-tensors-on-gpus"><a href="#2-1-tensors-on-gpus" class="headerlink" title="2.1 tensors on gpus"></a>2.1 tensors on gpus</h4><p>tensors默认存储在CPU里面，这时候计算速度非常慢。<br><code>x = toch.zero(32,32)</code><br>所以需要转移到GPU里面<br><code>y = x.to(&quot;cuda:0&quot;)</code><br>也可以直接在GPU里面创建tensors<br><code>z = torch.zeros(32, 32, device=&quot;cuda:0&quot;)</code><br>计算使用的显存<br><code>new_memory_allocated = torch.cuda.memory_allocated() # @inspect new_memory_allocated</code><br><code>memory_used = new_memory_allocated - memory_allocated # @inspect memory_used</code><br><code>assert memory_used == 2 * (32 * 32 * 4) # 2 32x32 matrices of 4-byte floats</code></p><p><img src="/img/lecture2_pytorch/Pasted%20image%2020260216133900.png" alt="图片"></p><h4 id="2-2-tensor-operations"><a href="#2-2-tensor-operations" class="headerlink" title="2.2 tensor_operations"></a>2.2 tensor_operations</h4><p><strong>PyTorch tensors are pointers into allocated memory</strong><br>用不同的strides来定位具体的元素<br><code>x.stride(0) == 4</code><br><code>x.stride(1) == 1</code><br>定位到6<br><code>r, c = 1, 2</code><br><code>index = r * x.stride(0) + c * x.stride(1) # @inspect index</code><br><code>assert index == 6</code><br><img src="/img/lecture2_pytorch/Pasted%20image%2020260216135100.png" alt="图片"><br>很多时候进行操作查看实际上是另一个视图而没有发生改变<br>如果说y是x的第一列，x和y实际上是一片存储空间，没有发生复制<br>如果转置后，变得不连续了，如果想变连续就会产生复制 contiguouos&#x2F;reshape（可能产生新空间）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python">x = torch.tensor([[<span class="hljs-number">1</span>,<span class="hljs-number">2</span>],[<span class="hljs-number">3</span>,<span class="hljs-number">4</span>]]) <br>x_t = x.t() <span class="hljs-comment"># 转置后不连续 </span><br><span class="hljs-built_in">print</span>(x_t.is_contiguous()) <span class="hljs-comment"># 输出: False</span><br>x_t_contig = x_t.contiguous() <span class="hljs-comment"># 复制内存，重新排列元素 </span><br><span class="hljs-built_in">print</span>(x_t_contig.is_contiguous()) <span class="hljs-comment"># 输出: True </span><br><span class="hljs-built_in">print</span>(x_t_contig.storage()) <span class="hljs-comment"># 内存存储变为 [1,3,2,4]（按转置后的形状重新排列）</span><br><br>x = torch.tensor([[<span class="hljs-number">1</span>,<span class="hljs-number">2</span>],[<span class="hljs-number">3</span>,<span class="hljs-number">4</span>]]) x_t = x.t() <span class="hljs-comment"># 不连续 </span><br>x_t_reshaped = x_t.reshape(<span class="hljs-number">4</span>) <span class="hljs-comment"># 会先复制内存 </span><br><span class="hljs-comment"># 验证：修改reshape后的张量，原张量不变（说明是复制） </span><br>x_t_reshaped[<span class="hljs-number">0</span>] = <span class="hljs-number">100</span> <br><span class="hljs-built_in">print</span>(x_t) <span class="hljs-comment"># 输出: tensor([[1, 3], [2, 4]])（原张量未变） </span><br><span class="hljs-built_in">print</span>(x_t_reshaped) <span class="hljs-comment"># 输出: tensor([100, 3, 2, 4])</span><br></code></pre></td></tr></table></figure><p><strong>矩阵乘法</strong><br>dl里面每个batch 每个sequence 对里面的每个token都做矩阵乘法</p><h4 id="2-3-tensor-einops"><a href="#2-3-tensor-einops" class="headerlink" title="2.3 tensor_einops"></a>2.3 tensor_einops</h4><ul><li><strong>jaxtyping</strong>：给张量维度做<strong>命名标注</strong>（如 <code>batch seq heads hidden</code>），代替纯数字 &#x2F; 注释，让维度含义一眼看懂，只是文档式说明，不强制校验。<br>  <img src="/img/lecture2_pytorch/Pasted%20image%2020260216145023.png" alt="图片"></li><li><strong>einops</strong>：一套统一、直观的张量操作库，替代易出错的 <code>reshape、transpose、dim=-1</code>：<ul><li><strong>einsum</strong>：用维度名写矩阵乘 &#x2F; 点积，不用转置，自动对重复维度求和，<code>...</code> 可通配任意前面维度；<br>  <img src="/img/lecture2_pytorch/Pasted%20image%2020260216145004.png" alt="图片"></li><li><strong>reduce</strong>：按名字对维度做 sum&#x2F;mean&#x2F;max 等聚合，不用记索引；<br>  <img src="/img/lecture2_pytorch/Pasted%20image%2020260216144941.png" alt="图片"></li><li><strong>rearrange</strong>：用括号轻松<strong>拆分 &#x2F; 合并维度</strong>（如把 <code>(heads hidden)</code> 拆成两个维度），完全替代复杂的 reshape + transpose<br>  <img src="/img/lecture2_pytorch/Pasted%20image%2020260216145040.png" alt="图片"></li></ul></li></ul><h4 id="2-4-tensor-operations-flops"><a href="#2-4-tensor-operations-flops" class="headerlink" title="2.4 tensor_operations_flops"></a>2.4 tensor_operations_flops</h4><ul><li><strong>FLOPs 浮点数运算次数：</strong><br>  两个矩阵相乘的时候：<br><code>x = torch.ones(B, D, device=device)</code><br><code>w = torch.randn(D, K, device=device)</code><br><code>y = x @ w</code><br><code>actual_num_flops = 2 * B * D * K</code><br>  最种输出的是B * K个数字，对每个数字，实际上就是i,j位置 j,k位置相乘了D次，然后相加了D-1次，工程上近似2D<br>  乘法最耗时，其它基础操作基本上是和矩阵大小（m * n）线性相关的</li><li><strong>FLOP&#x2F;s浮点数运算速度</strong></li><li><strong>MFU</strong>（model FLOPs utilization）：实际的FLOPs&#x2F;承诺的FLOPs       &gt;&#x3D;0.5比较好  衡量硬件能效</li></ul><h4 id="2-5-Gradients-basics-and-flops-梯度"><a href="#2-5-Gradients-basics-and-flops-梯度" class="headerlink" title="2.5 Gradients_basics and flops 梯度"></a>2.5 Gradients_basics and flops 梯度</h4><p><a href="https://medium.com/@dzmitrybahdanau/the-flops-calculus-of-language-model-training-3b19c1f025e4">https://medium.com/@dzmitrybahdanau/the-flops-calculus-of-language-model-training-3b19c1f025e4</a><br>一个动图（参考一下）</p><ul><li>向前传播 2倍datapoints * prameters</li><li>反向传播 4倍datapoints * prameters（链式求导）</li><li>总和 是6倍<br>  （ <strong>datapoints</strong>：对应之前的 <code>B</code>（batch size，批次样本数）；<br>   <strong>parameters</strong>：对应之前的 <code>D×K</code>（全连接层的参数总数，输入维度 D × 输出维度 K）；</li></ul><h3 id="3-Models"><a href="#3-Models" class="headerlink" title="3.Models"></a>3.Models</h3><h4 id="3-1-Parameter-initialization"><a href="#3-1-Parameter-initialization" class="headerlink" title="3.1 Parameter initialization"></a>3.1 Parameter initialization</h4><p>数值增长量级本质上和隐藏维度平方根成正比，大模型容易数值发散，不稳定<br>所以需要保证不容易爆炸，进行缩放<br><code>w = nn.Parameter(torch.randn(input_dim, output_dim) / np.sqrt(input_dim))</code><br>得到一个在（0,1)分布附近的数字。 Xavier initialization</p><h4 id="3-2-custom-model"><a href="#3-2-custom-model" class="headerlink" title="3.2 custom model"></a>3.2 custom model</h4><p>自定义模型，有一个矩阵层</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">assert</span> param_sizes == [<br><br>(<span class="hljs-string">&quot;layers.0.weight&quot;</span>, D * D),<br><br>(<span class="hljs-string">&quot;layers.1.weight&quot;</span>, D * D),<br><br>(<span class="hljs-string">&quot;final.weight&quot;</span>, D),<br><br>]<br><br>num_parameters = get_num_parameters(model)<br><br><span class="hljs-keyword">assert</span> num_parameters == (D * D) + (D * D) + D<br></code></pre></td></tr></table></figure><ul><li>关于随机性<ul><li>影响因素 ：初始化，dropout，数据排序，某些best practice</li><li>传播固定的seeds，便于重现，位不同随机来源设置不同的种子</li><li>让模型具有确定性比较好。<code>random.seed(seed)</code></li></ul></li></ul><h4 id="3-3-data-loading"><a href="#3-3-data-loading" class="headerlink" title="3.3 data loading"></a>3.3 data loading</h4><ul><li>data是一串有tokenizer输出的数字序列</li><li>分次加载到memory里面，使用memmap按需加载文件，分批采样加载数据</li></ul><h4 id="3-4-optimizer（优化器）"><a href="#3-4-optimizer（优化器）" class="headerlink" title="3.4 optimizer（优化器）"></a>3.4 optimizer（优化器）</h4><p>不同优化器</p><ul><li>AdaGrad 随机梯度下降<ul><li>momentum &#x3D; SGD + exponential averaging of grad 根据滑动平均值而不是瞬时平均值来更新</li><li>AdaGrad &#x3D; SGD + averaging by grad^2 根据历史梯度平方的平均值来缩放梯度</li><li>RMSProp &#x3D; AdaGrad + exponentially averaging of grad^2 指数滑动平方</li><li>Adam &#x3D; RMSProp + momentum 同时维护滑动平均值和梯度滑动平方平均值</li></ul></li><li>Adagrad的实现（override优化器类来实现）<a href="https://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf">https://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf</a><ol><li><strong>准备数据</strong>：生成随机输入张量<code>x</code>和目标值张量<code>y</code>，并指定运行设备（CPU&#x2F;GPU）；</li><li><strong>前向传播</strong>：将输入<code>x</code>传入模型<code>model</code>得到预测值<code>pred_y</code>；</li><li><strong>计算损失</strong>：用均方误差（MSE）计算预测值与目标值的损失<code>loss</code>；</li><li><strong>反向传播</strong>：通过<code>loss.backward()</code>计算模型参数的梯度；</li><li><strong>参数更新</strong>：调用<code>optimizer.step()</code>根据梯度更新模型参数；</li><li><strong>保存状态</strong>：提取并保存更新后的模型参数（<code>state_dict</code>）；</li><li><strong>清空梯度</strong>：可选操作，用<code>optimizer.zero_grad()</code>清空梯度缓存（<code>set_to_none=True</code>更节省内存）。</li></ol>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">step</span>(<span class="hljs-params">self</span>):<br><br><span class="hljs-keyword">for</span> group <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.param_groups:<br>lr = group[<span class="hljs-string">&quot;lr&quot;</span>]<br><span class="hljs-keyword">for</span> p <span class="hljs-keyword">in</span> group[<span class="hljs-string">&quot;params&quot;</span>]:<br><span class="hljs-comment"># Optimizer state</span><br>state = <span class="hljs-variable language_">self</span>.state[p]<br>grad = p.grad.data<br><span class="hljs-comment"># Get squared gradients g2 = sum_&#123;i&lt;t&#125; g_i^2</span><br>g2 = state.get(<span class="hljs-string">&quot;g2&quot;</span>, torch.zeros_like(grad))<br><span class="hljs-comment"># Update optimizer state</span><br>g2 += torch.square(grad)<br>state[<span class="hljs-string">&quot;g2&quot;</span>] = g2<br><span class="hljs-comment"># Update parameters</span><br>p.data -= lr * grad / torch.sqrt(g2 + <span class="hljs-number">1e-5</span>)<br></code></pre></td></tr></table></figure><ol><li><strong>遍历参数组与参数</strong>：外层循环遍历优化器的参数组（<code>param_groups</code>）获取学习率（<code>lr</code>），内层循环遍历每组中的模型参数（<code>params</code>）。</li><li><strong>初始化 &#x2F; 获取累计梯度平方</strong>：从参数的状态字典（<code>state</code>）中读取累计平方梯度<code>g2</code>，若不存在则初始化为与梯度同形状的零张量。</li><li><strong>更新累计梯度平方</strong>：将当前梯度的平方值累加到<code>g2</code>中，并更新回状态字典保存。</li><li><strong>更新模型参数</strong>：按照 Adagrad 公式更新参数，核心计算为<code>参数 = 参数 - 学习率 × 当前梯度 / (累计梯度平方和的平方根 + 极小值1e-5)</code>，其中<code>1e-5</code>用于避免分母为零。</li></ol></li><li>Memory 优化器的内存需求</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-comment"># Parameters</span><br>num_parameters = (D * D * num_layers) + D <span class="hljs-comment"># @inspect num_parameters</span><br><span class="hljs-keyword">assert</span> num_parameters == get_num_parameters(model)<br><span class="hljs-comment"># Activations</span><br>num_activations = B * D * num_layers <span class="hljs-comment"># @inspect num_activations</span><br><span class="hljs-comment"># Gradients</span><br>num_gradients = num_parameters <span class="hljs-comment"># @inspect num_gradients</span><br><span class="hljs-comment"># Optimizer states</span><br>num_optimizer_states = num_parameters <span class="hljs-comment"># @inspect num_optimizer_states</span><br><span class="hljs-comment"># Putting it all together, assuming float32</span><br>total_memory = <span class="hljs-number">4</span> * (num_parameters + num_activations + num_gradients + num_optimizer_states) <span class="hljs-comment"># @inspect total_memory</span><br></code></pre></td></tr></table></figure><pre><code class="hljs">1. 参数在显存中（矩阵参数量*层数+ D 是 bias 或 embedding 的小项。）2. forward 产生激活（存储每一层 forward产生的中间 hidden state后面求导要用，）a) B = batch size（或者 tokens per batch)b) D = hidden dimc) L = layers3. backward 产生梯度（梯度本来就是对每个参数求导）4. optimizer 用动量更新（看情况这里简约了）5. float32，4个字节</code></pre><h4 id="3-5-其它"><a href="#3-5-其它" class="headerlink" title="3.5 其它"></a>3.5 其它</h4><ul><li>激活值不一定要全部存储，在 backward 时可以通过重新 forward 计算恢复，有些激活值不能重新算（dropout这种有随机性的）</li><li>激活点检查：在 forward 阶段监控每一层输出值的数值分布，防止数值爆炸或塌缩影响梯度与优化稳定性。</li><li>checkpoint保存：训练过程中定期保存模型和优化器和迭代步数等，防止突然中断失去数据</li><li>训练的时候使用混合精度训练，可以在第五个forward用低一点的精度，训练其实对精度要求比较高，推理的时候可以通过量化模型来提升性能</li></ul>]]></content>
    
    
    <categories>
      
      <category>课程笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>CS336</tag>
      
      <tag>深度学习</tag>
      
      <tag>PyTorch</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CS336 Lecture 1 - 课程概要与tokenization历程</title>
    <link href="/2026/02/14/CS336-Lecture1/"/>
    <url>/2026/02/14/CS336-Lecture1/</url>
    
    <content type="html"><![CDATA[<p>本讲介绍 CS336 深度学习课程的整体内容，涵盖 LLM 模型的架构变体（Decoder-only、Encoder-only、Encoder-Decoder）以及对齐训练（Alignment）的方法论。重点讲解了两阶段训练流程：预训练（Pretraining）和有监督微调（Finetune），并深入探讨了强化学习在模型对齐中的应用（PPO、DPO、GRPO）。此外，详细介绍了 Tokenization 的演进历程，包括字符级、字节级、词级以及字节对编码（BPE）等不同方案，帮助理解文本如何转化为模型可处理的数字序列。</p><span id="more"></span><h2 id="1-课程内容"><a href="#1-课程内容" class="headerlink" title="1. 课程内容"></a>1. 课程内容</h2><p><img src="/img/Pasted%20image%2020260214151215.png" alt="课程概述"></p><h3 id="1-1-Architecture-变体"><a href="#1-1-Architecture-变体" class="headerlink" title="1.1 Architecture 变体"></a>1.1 Architecture 变体</h3><p><img src="/img/Pasted%20image%2020260214125753.png" alt="Architecture 变体"></p><h3 id="1-5-Alignment"><a href="#1-5-Alignment" class="headerlink" title="1.5 Alignment"></a>1.5 Alignment</h3><p>两阶段：</p><ul><li><strong>Pretraining</strong> - 预训练</li><li><strong>Finetune</strong> - 有监督的微调，使用 assistant pair 对</li></ul><p>算法角度：</p><ul><li>get more sft data 很昂贵，使用较轻的注释数据</li><li>使用偏好数据（A or B 哪个更好）- 另一种注释数据</li><li>或者和评估模型有关</li><li>强化学习：<ul><li>PPO - Proximal Policy Optimization（最古早) 奖励模型</li><li>DPO - Direct Policy Optimization 直接用偏好数据</li><li>GRPO - Group Relative Preference Optimization（Deepseek）- 组相对策略优化，无需训练奖励模型，直接通过群体对比输出进行优化</li></ul></li></ul><!-- more --><h2 id="2-Tokenizer"><a href="#2-Tokenizer" class="headerlink" title="2. Tokenizer"></a>2. Tokenizer</h2><p>字符串 → 一串数字序列 → 可以 decode 成字符串</p><ul><li><strong>Vocabulary size</strong> - 容量大小</li><li>从左到右来区分</li><li><strong>表达率</strong> - 每个 token 代表 1.66667 个字节的数据<br><img src="/img/Pasted%20image%2020260214144741.png" alt="表达率示例"></li></ul><h3 id="2-1-Character-based-tokenization"><a href="#2-1-Character-based-tokenization" class="headerlink" title="2.1 Character-based tokenization"></a>2.1 Character-based tokenization</h3><p>Unicode 编码的情况下，如果一个 character 对应一个 token，有 150k unicode 的字符：</p><ul><li>Vocabulary 很大</li><li>有一些不常见的字符比如表情符号，对 vocabulary 来说不是有效的利用</li></ul><h3 id="2-2-Byte-based-tokenization"><a href="#2-2-Byte-based-tokenization" class="headerlink" title="2.2 Byte-based tokenization"></a>2.2 Byte-based tokenization</h3><h3 id="2-3-Word-based"><a href="#2-3-Word-based" class="headerlink" title="2.3 Word-based"></a>2.3 Word-based</h3><p><img src="/img/Pasted%20image%2020260214145503.png" alt="Word-based 示例"></p><h3 id="2-4-Byte-Pair-Encoding-BPE"><a href="#2-4-Byte-Pair-Encoding-BPE" class="headerlink" title="2.4 Byte Pair Encoding (BPE)"></a>2.4 Byte Pair Encoding (BPE)</h3><p>之前都是用的基于单词的，94 年开始用的这个算法。GPT2 用的就是这个 BPE。</p><p><strong>算法原理</strong>：</p><ul><li>Indices 里面相邻的数字 pair 出现最多次就将这两个合并成一个数字</li><li>合并多轮</li></ul><p><img src="/img/Pasted%20image%2020260214150144.png" alt="BPE 算法示例"></p><hr><blockquote><p>课程笔记，持续更新中…</p></blockquote>]]></content>
    
    
    <categories>
      
      <category>课程笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>CS336</tag>
      
      <tag>深度学习</tag>
      
      <tag>LLM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>第三篇博客：AICoding 工具体验（Cursor / Trae / Antigravity / Windsurf + Claude Code / Open Code）</title>
    <link href="/2026/01/09/%E7%AC%AC%E4%B8%89%E7%AF%87%E5%8D%9A%E5%AE%A2-AICoding%E5%B7%A5%E5%85%B7%E4%BD%93%E9%AA%8C/"/>
    <url>/2026/01/09/%E7%AC%AC%E4%B8%89%E7%AF%87%E5%8D%9A%E5%AE%A2-AICoding%E5%B7%A5%E5%85%B7%E4%BD%93%E9%AA%8C/</url>
    
    <content type="html"><![CDATA[<p>这篇是我的第三篇博客，准备系统性更新我对一批「AI coding 编译器 &#x2F; AI 编程工具」的使用体验与选型思路：Cursor、Trae、Antigravity、Windsurf，以及终端侧的 Claude Code 和 Open Code。</p><span id="more"></span><h2 id="1-我说的“AI-coding-编译器”到底是什么"><a href="#1-我说的“AI-coding-编译器”到底是什么" class="headerlink" title="1. 我说的“AI coding 编译器”到底是什么"></a>1. 我说的“AI coding 编译器”到底是什么</h2><p>依据简介：把它当作“把需求编译成可运行代码”的新工作台：不仅是补全&#x2F;聊天，而是集成了上下文检索、任务分解、多文件修改、运行与回归验证的完整链路。</p><h3 id="1-1-它和网页端-Gemini（Canvas-模式）生成内容有什么区别"><a href="#1-1-它和网页端-Gemini（Canvas-模式）生成内容有什么区别" class="headerlink" title="1.1 它和网页端 Gemini（Canvas 模式）生成内容有什么区别"></a>1.1 它和网页端 Gemini（Canvas 模式）生成内容有什么区别</h3><p>依据简介：两者都能“产出代码&#x2F;文档”，但默认目标完全不同：</p><ul><li>网页端 Canvas 更像是“内容生成与排版&#x2F;编辑工作台”，擅长把想法快速写成一份可阅读、可复制的文本产物。</li><li>AI coding 编译器更像是“在你的代码库里完成一次可落地的变更”，核心是把产物变成仓库里的真实修改，并把结果跑通。</li></ul><h3 id="1-2-关键差异-1：上下文来源不同："><a href="#1-2-关键差异-1：上下文来源不同：" class="headerlink" title="1.2 关键差异 1：上下文来源不同："></a>1.2 关键差异 1：上下文来源不同：</h3><p>Canvas 通常以你粘贴&#x2F;上传的材料为主，上下文是“片段式”的；<br>编译器类工具更强调索引整个仓库（目录结构、依赖、类型、调用链、已有约束），能在更大范围内做一致性修改。</p><h3 id="1-3-关键差异-2：产物形态不同（文本结果-vs-可审查的-Diff）"><a href="#1-3-关键差异-2：产物形态不同（文本结果-vs-可审查的-Diff）" class="headerlink" title="1.3 关键差异 2：产物形态不同（文本结果 vs 可审查的 Diff）"></a>1.3 关键差异 2：产物形态不同（文本结果 vs 可审查的 Diff）</h3><p>Canvas 的典型产物是一段&#x2F;一页内容；你需要自己把它“搬运”进项目。编译器类工具的产物更接近 Git 工作流：以文件改动（diff）呈现，便于审查、回滚、分批合并。</p><h3 id="1-4-关键差异-3：闭环能力不同（会写-vs-能跑）"><a href="#1-4-关键差异-3：闭环能力不同（会写-vs-能跑）" class="headerlink" title="1.4 关键差异 3：闭环能力不同（会写 vs 能跑）"></a>1.4 关键差异 3：闭环能力不同（会写 vs 能跑）</h3><p>Canvas 很容易给出“看起来合理”的方案，但默认不负责把它跑通。<br>编译器类工具更强调闭环：改代码 -&gt; 运行&#x2F;构建&#x2F;测试 -&gt; 根据报错继续修 -&gt; 直到通过（至少在流程上更顺）。</p><h3 id="1-5-关键差异-4：约束与安全边界不同（复制粘贴可控-vs-自动改动需设闸）"><a href="#1-5-关键差异-4：约束与安全边界不同（复制粘贴可控-vs-自动改动需设闸）" class="headerlink" title="1.5 关键差异 4：约束与安全边界不同（复制粘贴可控 vs 自动改动需设闸）"></a>1.5 关键差异 4：约束与安全边界不同（复制粘贴可控 vs 自动改动需设闸）</h3><p>Canvas 的复制粘贴由人工操控；编译器类工具可以直接改多文件、甚至执行命令，所以更需要你设置验收点（先 plan、再 diff 审核、再运行验证），否则更容易在代码库里引入隐性破坏。</p><h3 id="1-6-我自己的用法建议：什么时候用-Canvas，什么时候用“编译器”"><a href="#1-6-我自己的用法建议：什么时候用-Canvas，什么时候用“编译器”" class="headerlink" title="1.6 我自己的用法建议：什么时候用 Canvas，什么时候用“编译器”"></a>1.6 我自己的用法建议：什么时候用 Canvas，什么时候用“编译器”</h3><ul><li>用 Canvas：演示的demo。</li><li>用 AI coding 编译器：在真实项目里落地需求（跨文件改动&#x2F;重构&#x2F;补测试&#x2F;修 bug）、需要跑通构建与测试、需要对修改进行审查与迭代，有前后端。</li></ul><h2 id="2-选型框架：比较工具前先比较你的场景"><a href="#2-选型框架：比较工具前先比较你的场景" class="headerlink" title="2. 选型框架：比较工具前先比较你的场景"></a>2. 选型框架：比较工具前先比较你的场景</h2><p>依据简介：不同工具的强弱往往取决于你的约束（个人&#x2F;团队、代码库规模、语言栈、是否可联网、是否允许上传代码、预算&#x2F;延迟）。这一节先把约束列清楚，避免“看起来都很强”但落地失败。</p><h2 id="3-通用评价维度：我用这-8-个指标做对比"><a href="#3-通用评价维度：我用这-8-个指标做对比" class="headerlink" title="3. 通用评价维度：我用这 8 个指标做对比"></a>3. 通用评价维度：我用这 8 个指标做对比</h2><p>依据简介：用统一的“评分尺子”对齐主观体验，便于复用到任何新工具：</p><ul><li>上下文能力（索引、检索、引用是否可信）</li><li>多文件&#x2F;多步骤任务能力（agent 能否持续推进）</li><li>可控性与可回滚（diff、plan、审批点）</li><li>运行&#x2F;测试闭环（是否把你带到“能跑”而不是“看起来对”）</li><li>代码质量（风格一致性、架构意识、边界处理）</li><li>成本与速度（token&#x2F;订阅、响应时间）</li><li>生态与可扩展（插件、模型选择、规则系统）</li><li>隐私与合规（上传范围、日志、企业策略）</li></ul><h2 id="4-Cursor：从“VS-Code-AI”到日常主力的理由"><a href="#4-Cursor：从“VS-Code-AI”到日常主力的理由" class="headerlink" title="4. Cursor：从“VS Code + AI”到日常主力的理由"></a>4. Cursor：从“VS Code + AI”到日常主力的理由</h2><p>依据简介：重点记录它在日常开发里的高频价值点（如：代码库理解、稳定的多文件改动、可维护的修改方式），以及什么时候会让你“越用越快”。</p><h2 id="5-Trae：我把它放进对比的原因与观察重点"><a href="#5-Trae：我把它放进对比的原因与观察重点" class="headerlink" title="5. Trae：我把它放进对比的原因与观察重点"></a>5. Trae：我把它放进对比的原因与观察重点</h2><p>依据简介：我会从“定位&#x2F;工作流&#x2F;上下文&#x2F;运行闭环&#x2F;生态”五个角度记录 Trae 的优劣；这一节更关注它是否能在你的真实工程里减少切换成本、以及对中文需求表达是否更友好。</p><h2 id="6-Antigravity：更偏“任务驱动-代理式”的使用方式"><a href="#6-Antigravity：更偏“任务驱动-代理式”的使用方式" class="headerlink" title="6. Antigravity：更偏“任务驱动&#x2F;代理式”的使用方式"></a>6. Antigravity：更偏“任务驱动&#x2F;代理式”的使用方式</h2><p>依据简介：这类工具通常更强调把需求拆成任务并自动推进；我会重点记录它在复杂需求（跨文件改动、重构、补测试）上的推进效率，以及如何设置边界来避免跑偏。</p><h2 id="7-Windsurf：IDE-内的-Agent-工作流（以及我如何用它）"><a href="#7-Windsurf：IDE-内的-Agent-工作流（以及我如何用它）" class="headerlink" title="7. Windsurf：IDE 内的 Agent 工作流（以及我如何用它）"></a>7. Windsurf：IDE 内的 Agent 工作流（以及我如何用它）</h2><p>依据简介：我会把 Windsurf 当成“更强的 agent 工作台”来写，重点放在：如何给 agent 足够上下文、如何分阶段验收、以及它在多轮迭代时是否稳定。</p><h2 id="8-终端派：Claude-Code-与-Open-Code-的“CLI-工作流”"><a href="#8-终端派：Claude-Code-与-Open-Code-的“CLI-工作流”" class="headerlink" title="8. 终端派：Claude Code 与 Open Code 的“CLI 工作流”"></a>8. 终端派：Claude Code 与 Open Code 的“CLI 工作流”</h2><p>依据简介：当你更想用脚本&#x2F;命令驱动仓库操作（跑测试、改配置、批量重构）时，CLI 工具往往更顺手；我会记录它们适合的任务类型、提示词习惯、以及和 IDE 工具如何互补。</p><ul><li><p>Claude Code<br>实用性入门网站 </p></li><li><p>安装教程 <a href="https://claudecode.io/zh/instal">https://claudecode.io/zh/instal</a></p></li><li><p>学习教程 <a href="https://claudecode.tangshuang.net/">https://claudecode.tangshuang.net/</a></p></li><li><p>Open Code</p></li></ul><h2 id="9-同一个需求的对照实验：6-个工具分别怎么做"><a href="#9-同一个需求的对照实验：6-个工具分别怎么做" class="headerlink" title="9. 同一个需求的对照实验：6 个工具分别怎么做"></a>9. 同一个需求的对照实验：6 个工具分别怎么做</h2><p>依据简介：用同一个小项目需求做对照，才看得出差异：</p><ul><li>需求理解与澄清能力</li><li>计划（plan）质量</li><li>多文件修改稳定性</li><li>自检（lint&#x2F;test）与回归</li><li>最终交付的可维护性</li></ul><h2 id="10-最容易踩坑的-10-件事（以及我的规避策略）"><a href="#10-最容易踩坑的-10-件事（以及我的规避策略）" class="headerlink" title="10. 最容易踩坑的 10 件事（以及我的规避策略）"></a>10. 最容易踩坑的 10 件事（以及我的规避策略）</h2><p>依据简介：把“体验差”的根因归类（上下文不足、目标不清、一次给太大任务、缺少验收点、没让它跑测试等），并给出我实际可复用的规避手法。</p><h2 id="11-我的结论：不同人群-不同项目的推荐组合"><a href="#11-我的结论：不同人群-不同项目的推荐组合" class="headerlink" title="11. 我的结论：不同人群&#x2F;不同项目的推荐组合"></a>11. 我的结论：不同人群&#x2F;不同项目的推荐组合</h2><p>依据简介：给出一个“按场景选工具”的结论，而不是单纯排名：</p><ul><li>新手入门</li><li>独立开发&#x2F;快速迭代</li><li>中大型代码库</li><li>团队协作&#x2F;企业合规</li></ul><h2 id="12-后续更新计划（持续补充）"><a href="#12-后续更新计划（持续补充）" class="headerlink" title="12. 后续更新计划（持续补充）"></a>12. 后续更新计划（持续补充）</h2><p>依据简介：这篇会持续更新，我会补：更真实的案例、每个工具的配置与模板、以及我认为最值得抄作业的工作流。</p>]]></content>
    
    
    <categories>
      
      <category>动手做些事情吧~</category>
      
    </categories>
    
    
    <tags>
      
      <tag>AICoding</tag>
      
      <tag>工具</tag>
      
      <tag>工作流</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>第一篇博客</title>
    <link href="/2025/12/19/%E7%AC%AC%E4%B8%80%E7%AF%87%E5%8D%9A%E5%AE%A2/"/>
    <url>/2025/12/19/%E7%AC%AC%E4%B8%80%E7%AF%87%E5%8D%9A%E5%AE%A2/</url>
    
    <content type="html"><![CDATA[<p>这是我的第一篇博客，就简单写一些搭建博客的内容吧！<br>本文主要是关于搭建博客&#x2F;修饰博客&#x2F;实现某些小功能。</p><span id="more"></span><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><p>目前需要一个占位符QQ…</p><h3 id="1-搭建博客"><a href="#1-搭建博客" class="headerlink" title="1. 搭建博客"></a>1. 搭建博客</h3><h4 id="1-1-Github相关准备"><a href="#1-1-Github相关准备" class="headerlink" title="1.1 Github相关准备"></a>1.1 Github相关准备</h4><h4 id="1-2-Hexo相关准备"><a href="#1-2-Hexo相关准备" class="headerlink" title="1.2 Hexo相关准备"></a>1.2 Hexo相关准备</h4><h4 id="1-3-博客初始化与部署"><a href="#1-3-博客初始化与部署" class="headerlink" title="1.3 博客初始化与部署"></a>1.3 博客初始化与部署</h4><h4 id="1-4-博客推送"><a href="#1-4-博客推送" class="headerlink" title="1.4 博客推送"></a>1.4 博客推送</h4><h3 id="2-修饰博客主题"><a href="#2-修饰博客主题" class="headerlink" title="2. 修饰博客主题"></a>2. 修饰博客主题</h3><h4 id="2-1-改变颜色"><a href="#2-1-改变颜色" class="headerlink" title="2.1 改变颜色"></a>2.1 改变颜色</h4><h4 id="2-2-更换图片"><a href="#2-2-更换图片" class="headerlink" title="2.2 更换图片"></a>2.2 更换图片</h4><h3 id="3-丰富博客内容"><a href="#3-丰富博客内容" class="headerlink" title="3. 丰富博客内容"></a>3. 丰富博客内容</h3><h4 id="3-1-实现其它网页"><a href="#3-1-实现其它网页" class="headerlink" title="3.1 实现其它网页"></a>3.1 实现其它网页</h4><h4 id="3-2-添加评论功能"><a href="#3-2-添加评论功能" class="headerlink" title="3.2 添加评论功能"></a>3.2 添加评论功能</h4><hr><blockquote><p>Running step by step.</p></blockquote>]]></content>
    
    
    <categories>
      
      <category>动手做些事情吧~</category>
      
    </categories>
    
    
    <tags>
      
      <tag>技术</tag>
      
      <tag>博客</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>博客更新计划</title>
    <link href="/2025/12/19/%E5%8D%9A%E5%AE%A2%E6%9B%B4%E6%96%B0%E8%AE%A1%E5%88%92/"/>
    <url>/2025/12/19/%E5%8D%9A%E5%AE%A2%E6%9B%B4%E6%96%B0%E8%AE%A1%E5%88%92/</url>
    
    <content type="html"><![CDATA[<p>这是我的博客内容更新计划，我会按照这个列表逐步填充我的个人空间。</p><span id="more"></span><h3 id="核心专栏"><a href="#核心专栏" class="headerlink" title="核心专栏"></a>核心专栏</h3><ul><li><input disabled="" type="checkbox"> <strong>第一篇博客</strong> —— 关于博客搭建</li><li><input disabled="" type="checkbox"> <strong>VibeCoding 系列</strong><ul><li><input disabled="" type="checkbox"> 纯小白如何从0-1实现 VibeCoding</li><li><input disabled="" type="checkbox"> 自己的文献阅读产品 - SciReader</li></ul></li><li><input disabled="" type="checkbox"> <strong>知识管理</strong> —— 如何构建自己的知识体系 (Obsidian &#x2F; Flowus &#x2F; Zotero 等产品分享)</li><li><input disabled="" type="checkbox"> <strong>AICoding</strong> —— Cursor &#x2F; Antigravity &#x2F; Trae 使用经验</li></ul><hr><h3 id="兴趣爱好系列-点击展开详情"><a href="#兴趣爱好系列-点击展开详情" class="headerlink" title="兴趣爱好系列 (点击展开详情)"></a>兴趣爱好系列 (点击展开详情)</h3><details><summary><b>📷 摄影</b></summary><ul><li>手机摄影（Tim49元课程笔记）</li><li>微单摄影（Sony A6400学习笔记）</li><li>胶片摄影 (一次性胶片机与傻瓜机使用记录)</li></ul></details><details><summary><b>🎹 钢琴</b></summary><ul><li>重拾记录</li><li>练习记录<ul><li>2025.12.12 - 待定 《圣诞快乐！劳伦斯先生》</li></ul></li></ul></details><details><summary><b>🧶 针织</b></summary><ul><li>入门记录</li><li>作品展示<ul><li>雪花 ❄️</li></ul></li></ul></details><hr><blockquote><p>Running step by step.</p></blockquote>]]></content>
    
    
    <categories>
      
      <category>杂谈</category>
      
    </categories>
    
    
    <tags>
      
      <tag>计划</tag>
      
      <tag>预览</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
